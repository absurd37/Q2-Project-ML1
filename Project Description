The k-Nearest Neighbors (kNN) algorithm is widely used for classification due to its simplicity and effectiveness. However, traditional kNN suffers from limitations such as being unable to adjust to local neighborhood density and equal feature weighting in distance calculations, which means less relevant features are equally considered. This paper introduces an adaptive kNN variant that integrates local k selection, information-based feature weighting, and the time-efficient Ball Tree method to improve classification performance. Feature importance is determined using a combination of Mutual Information (MI) and Shapley values to ensure that more informative features contribute more significantly to distance calculations. In order to be flexible to local conditions around an instance, the algorithm dynamically selects a local k value for each test instance based on the consistency of its nearest neighbors to prevent misclassification in heterogeneous datasets. The final classification is performed using majority voting among the k nearest neighbors. Experimental results demonstrate that the proposed approach outperforms traditional kNN and standard feature-weighted kNN methods on the tested UCI Machine Learning datasets, Mammographic Mass and Blood Transfusion Service Center. These findings suggest that incorporating adaptive feature weighting, local k-selection, and probabilistic feature importance estimation significantly enhances kNNâ€™s flexibility and classification accuracy.
